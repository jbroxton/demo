/**
 * @fileoverview Speqq AI Chat Feature - Implementation Overview
 * 
 * @description The Speqq AI feature provides intelligent chat assistance for product management 
 * workflows. Users can query their product data using natural language and get contextual 
 * responses about features, releases, and requirements. The system offers two distinct AI modes 
 * to meet different use cases.
 * 
 * @version 1.2.0
 * @since 1.0.0
 * @lastUpdated 2025-01-28
 * 
 * @architecture
 * Mode 1 - RAG Chat:
 * User Query → Vector Search (Supabase pgvector) → Context Retrieval → OpenAI GPT + Context → Response
 * 
 * Mode 2 - OpenAI Fully Managed:
 * User Message → OpenAI Thread → OpenAI Assistant + File Search → Response
 * 
 * @features
 * ✅ **Dual AI Modes**
 * - RAG-based chat using custom vector search
 * - OpenAI Assistants with fully managed file search
 * - User can toggle between modes in real-time
 * 
 * ✅ **Natural Language Querying**
 * - Semantic search across product data
 * - Intelligent query type detection (count, list, search, specific)
 * - Dynamic result limiting based on query intent
 * 
 * ✅ **Multi-Tenant Architecture** 
 * - Complete data isolation per tenant
 * - Row-level security in database
 * - Tenant-specific OpenAI assistants and files
 * 
 * ✅ **Auto-Embedding System**
 * - Automatic embedding generation via database triggers
 * - Background queue processing with pgmq
 * - Manual sync fallback for immediate indexing
 * 
 * ✅ **Real-Time Chat Interface**
 * - Streaming responses via Vercel AI SDK
 * - Message persistence and history
 * - Conversation threads in OpenAI mode
 * 
 * @entities Currently Supported:
 * - **Features**: Query name, priority, status, description, requirements
 * - **Releases**: Query release dates, priorities, descriptions
 * - Limited support for requirements and roadmaps
 * 
 * @capabilities
 * **RAG Mode Capabilities:**
 * - Semantic search across embedded product data
 * - Specific entity queries: "What features do I have with high priority?"
 * - Counting queries: "How many active features are there?"
 * - Filtering: "List all features for mobile interface"
 * - Search: "Find authentication related features"
 * - Real-time access to current database state
 * - Supports: Features, Releases (embedded automatically)
 * 
 * **OpenAI Fully Managed Mode Capabilities:**
 * - Natural conversation with memory across messages
 * - General product management advice and guidance
 * - File-based context from uploaded tenant data
 * - More sophisticated reasoning and analysis
 * - Conversational follow-ups and clarifications
 * - Supports: Any data included in uploaded files
 * 
 * **Mode Comparison:**
 * - RAG Mode: Real-time database queries, specific data retrieval, no conversation memory
 * - OpenAI Mode: Conversational AI, file-based context, persistent threads, broader reasoning
 * 
 * @performance
 * - Average response time: 2-3 seconds
 * - Vector search: ~200ms with HNSW indexing
 * - Embedding generation: ~500ms per query
 * - Supports 100+ concurrent users
 * - Dynamic limits: 10-50 results based on query type
 * 
 * @technical
 * **Frontend:**
 * - React components with Vercel AI SDK
 * - Real-time streaming responses
 * - Mode switching UI with status indicators
 * 
 * **Backend:**
 * - Next.js API routes (/api/ai-chat, /api/ai-chat-fully-managed)
 * - OpenAI GPT-4 for text generation
 * - OpenAI text-embedding-3-small for vectors
 * - Supabase pgvector for similarity search
 * 
 * **Database:**
 * - ai_embeddings: Vector storage with 1536-dim embeddings
 * - ai_messages: Chat history and persistence  
 * - ai_sessions: User session management
 * - Auto-embedding triggers on data changes
 * 
 * @limitations
 * **RAG Mode Limitations:**
 * - No conversation memory between messages
 * - Limited to embedded entities only (Features, Releases)
 * - Cannot provide general product management advice
 * - Responses are data-focused, not conversational
 * - No follow-up context from previous questions
 * 
 * **OpenAI Fully Managed Mode Limitations:**
 * - Requires manual file sync for up-to-date context
 * - Context depends on uploaded file contents
 * - May not have real-time database state
 * - Cannot perform specific database queries
 * - Limited by file upload and processing delays
 * 
 * **Both Modes:**
 * - Read-only queries (no data modification yet)
 * - No agent actions for updating features/releases
 * - No integration with external tools
 * 
 * @files
 * **Core Implementation:**
 * - `src/components/ai-chat/index.tsx` - Main chat UI component
 * - `src/app/api/ai-chat/route.ts` - RAG chat API endpoint
 * - `src/app/api/ai-chat-fully-managed/route.ts` - OpenAI Assistants API
 * - `src/services/ai-service.ts` - Vector search and embedding service
 * - `src/hooks/use-ai-chat.ts` - RAG chat hook
 * - `src/hooks/use-ai-chat-fully-managed.ts` - OpenAI chat hook
 * 
 * **Database Schema:**
 * - `supabase/migrations/setup_auto_embeddings.sql` - Vector database setup
 * - Database tables: ai_embeddings, ai_messages, ai_sessions
 * 
 * @examples
 * ```typescript
 * // RAG Mode Usage
 * import { useAIChat } from '@/hooks/use-ai-chat';
 * const { messages, input, handleSubmit, isLoading } = useAIChat();
 * 
 * // OpenAI Mode Usage  
 * import { useAiChatFullyManaged } from '@/hooks/use-ai-chat-fully-managed';
 * const { sendMessage, messages, isLoading } = useAiChatFullyManaged();
 * 
 * // Vector Search Service
 * import { searchVectors } from '@/services/ai-service';
 * const results = await searchVectors("high priority features", tenantId);
 * ```
 * 
 * @testing
 * - Unit tests for vector search functionality
 * - Integration tests for API endpoints
 * - Manual testing for chat UI interactions
 * 
 * @monitoring
 * - OpenAI API usage tracking
 * - Vector search performance metrics
 * - Error logging and debugging
 * 
 * @security
 * - Multi-tenant data isolation via RLS policies
 * - Authentication required for all AI endpoints
 * - Input validation with Zod schemas
 * - Environment variable protection for API keys
 * 
 * @deployment
 * - Hosted on Vercel with Next.js App Router
 * - Supabase for database and vector search
 * - OpenAI API for language models and embeddings
 * - Auto-scaling with serverless functions
 * 
 * @changelog
 * **v1.2.0 (2025-01-28)**
 * - Added OpenAI Fully Managed mode
 * - Implemented auto-embedding system
 * - Enhanced chat UI with mode switching
 * - Improved vector search with dynamic limits
 * 
 * **v1.1.0 (2025-01-15)**
 * - Added intelligent query type detection
 * - Implemented keyword re-ranking
 * - Enhanced error handling and logging
 * 
 * **v1.0.0 (2025-01-01)**
 * - Initial RAG implementation
 * - Basic vector search functionality
 * - Multi-tenant architecture
 * - Chat UI with streaming responses
 * 
 * @roadmap
 * **Next Release (v1.3.0):**
 * - Agent actions for data modification
 * - Support for additional entities (roadmaps, goals)
 * - Conversation history management
 * - Advanced analytics and insights
 * 
 * **Future Considerations:**
 * - Custom trained models for product management
 * - Integration with external tools (Slack, Jira)
 * - Voice input and audio responses
 * - Advanced visualization of query results
 * 
 * @maintainers
 * - Primary: AI/Product team
 * - Secondary: Full-stack developers
 * - Review: Product management stakeholders
 * 
 * ---
 * 
 * This document serves as the **source of truth** for the Speqq AI feature.
 * Update this file whenever new functionality is added or existing features change.
 * All other AI documentation should reference this file for current implementation status.
 */



